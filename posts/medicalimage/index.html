<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>医疗影像网络架构学习 | perhapzz</title>
<meta name="keywords" content="">
<meta name="description" content="ViT Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Paper: arXiv:2010.11929
 把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。
ViT 结构 按照上面的流程图，一个 ViT block 可以分为以下几个步骤:
 patch embedding：例如输入图片大小为 224x224，将图片分为固定大小的 patch，patch 大小为 16x16，则每张图像会生成 224x224/16x16=196 个 patch，即输入序列长度为 196，每个patch维度 16x16x3=768，线性投射层的维度为 768xN (N=768)，因此输入通过线性投射层之后的维度依然为 196x768，即一共有 196 个 token，每个 token 的维度是 768。这里还需要加上一个特殊字符 cls（类似 BERT），因此最终的维度是 197x768。到目前为止，已经通过 patch embedding 将一个视觉问题转化为了一个 seq2seq 问题。 positional encoding（standard learnable 1D position embeddings）：ViT 同样需要加入位置编码，位置编码可以理解为一张表，表一共有 N 行，N 的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列 embedding 的维度相同（768）。注意位置编码的操作是 sum，而不是 concat。加入位置编码信息之后，维度依然是 197x768。 LN/multi-head attention/LN：LN输出维度依然是 197x768。多头自注意力时，先将输入映射到 q，k，v，如果只有一个头，qkv 的维度都是197x768，如果有 12 个头（768/12=64），则 qkv 的维度是 197x64，一共有 12 组 qkv，最后再将 12 组 qkv 的输出拼接起来，输出维度是 197x768，然后在过一层 LN，维度依然是197x768。 MLP：将维度放大再缩小回去，197x768 放大为 197x3072，再缩小变为 197x768。  一个 block 之后维度依然和输入相同，都是 197x768，因此可以堆叠多个 block。最后会将特殊字符 cls 对应的输出 $Z_L^0$ 作为 encoder 的最终输出 ，代表最终的 image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），如下公式(4)，后面接一个MLP进行图片分类： $$ \begin{aligned} \mathbf{z}{0} &amp; =\left[\mathbf{x}{\text {class }} ; \mathbf{x}{p}^{1} \mathbf{E} ; \mathbf{x}{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}{p}^{N} \mathbf{E}\right]&#43;\mathbf{E}{\text {pos }}, &amp; &amp; \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}{\text {pos }} \in \mathbb{R}^{(N&#43;1) \times D} \ \mathbf{z}{\ell}^{\prime} &amp; =\operatorname{MSA}\left(\mathrm{LN}\left(\mathbf{z}{\ell-1}\right)\right)&#43;\mathbf{z}{\ell-1}, &amp; &amp; \ell=1 \ldots L \ \mathbf{z}{\ell} &amp; =\operatorname{MLP}\left(\mathrm{LN}\left(\mathbf{z}{\ell}^{\prime}\right)\right)&#43;\mathbf{z}{\ell}^{\prime}, &amp; &amp; \ell=1 \ldots L \ \mathbf{y} &amp; =\operatorname{LN}\left(\mathbf{z}{L}^{0}\right) &amp; &amp; \end{aligned} $$ 其中输入 image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ ，2D patches $\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$ ， $C$ 是通道数，$P$ 是 patch 大小，一共有 $N$ 个 patches，$N = HW/P^2$ 。">
<meta name="author" content="">
<link rel="canonical" href="https://perhapzz.github.io/posts/medicalimage/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.2c5d624023ca4f39285dc8ad242509c565a0c0e820fc61724b2a58620dfdafbe.css" integrity="sha256-LF1iQCPKTzkoXcitJCUJxWWgwOgg/GFySypYYg39r74=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://perhapzz.github.io/favicon/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://perhapzz.github.io/favicon/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://perhapzz.github.io/favicon/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://perhapzz.github.io/favicon/apple-touch-icon.png">
<link rel="mask-icon" href="https://perhapzz.github.io/favicon/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:title" content="医疗影像网络架构学习" />
<meta property="og:description" content="ViT Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Paper: arXiv:2010.11929
 把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。
ViT 结构 按照上面的流程图，一个 ViT block 可以分为以下几个步骤:
 patch embedding：例如输入图片大小为 224x224，将图片分为固定大小的 patch，patch 大小为 16x16，则每张图像会生成 224x224/16x16=196 个 patch，即输入序列长度为 196，每个patch维度 16x16x3=768，线性投射层的维度为 768xN (N=768)，因此输入通过线性投射层之后的维度依然为 196x768，即一共有 196 个 token，每个 token 的维度是 768。这里还需要加上一个特殊字符 cls（类似 BERT），因此最终的维度是 197x768。到目前为止，已经通过 patch embedding 将一个视觉问题转化为了一个 seq2seq 问题。 positional encoding（standard learnable 1D position embeddings）：ViT 同样需要加入位置编码，位置编码可以理解为一张表，表一共有 N 行，N 的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列 embedding 的维度相同（768）。注意位置编码的操作是 sum，而不是 concat。加入位置编码信息之后，维度依然是 197x768。 LN/multi-head attention/LN：LN输出维度依然是 197x768。多头自注意力时，先将输入映射到 q，k，v，如果只有一个头，qkv 的维度都是197x768，如果有 12 个头（768/12=64），则 qkv 的维度是 197x64，一共有 12 组 qkv，最后再将 12 组 qkv 的输出拼接起来，输出维度是 197x768，然后在过一层 LN，维度依然是197x768。 MLP：将维度放大再缩小回去，197x768 放大为 197x3072，再缩小变为 197x768。  一个 block 之后维度依然和输入相同，都是 197x768，因此可以堆叠多个 block。最后会将特殊字符 cls 对应的输出 $Z_L^0$ 作为 encoder 的最终输出 ，代表最终的 image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），如下公式(4)，后面接一个MLP进行图片分类： $$ \begin{aligned} \mathbf{z}{0} &amp; =\left[\mathbf{x}{\text {class }} ; \mathbf{x}{p}^{1} \mathbf{E} ; \mathbf{x}{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}{p}^{N} \mathbf{E}\right]&#43;\mathbf{E}{\text {pos }}, &amp; &amp; \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}{\text {pos }} \in \mathbb{R}^{(N&#43;1) \times D} \ \mathbf{z}{\ell}^{\prime} &amp; =\operatorname{MSA}\left(\mathrm{LN}\left(\mathbf{z}{\ell-1}\right)\right)&#43;\mathbf{z}{\ell-1}, &amp; &amp; \ell=1 \ldots L \ \mathbf{z}{\ell} &amp; =\operatorname{MLP}\left(\mathrm{LN}\left(\mathbf{z}{\ell}^{\prime}\right)\right)&#43;\mathbf{z}{\ell}^{\prime}, &amp; &amp; \ell=1 \ldots L \ \mathbf{y} &amp; =\operatorname{LN}\left(\mathbf{z}{L}^{0}\right) &amp; &amp; \end{aligned} $$ 其中输入 image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ ，2D patches $\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$ ， $C$ 是通道数，$P$ 是 patch 大小，一共有 $N$ 个 patches，$N = HW/P^2$ 。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://perhapzz.github.io/posts/medicalimage/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-03-22T21:19:02+08:00" />
<meta property="article:modified_time" content="2023-03-22T21:19:02+08:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="医疗影像网络架构学习"/>
<meta name="twitter:description" content="ViT Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Paper: arXiv:2010.11929
 把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。
ViT 结构 按照上面的流程图，一个 ViT block 可以分为以下几个步骤:
 patch embedding：例如输入图片大小为 224x224，将图片分为固定大小的 patch，patch 大小为 16x16，则每张图像会生成 224x224/16x16=196 个 patch，即输入序列长度为 196，每个patch维度 16x16x3=768，线性投射层的维度为 768xN (N=768)，因此输入通过线性投射层之后的维度依然为 196x768，即一共有 196 个 token，每个 token 的维度是 768。这里还需要加上一个特殊字符 cls（类似 BERT），因此最终的维度是 197x768。到目前为止，已经通过 patch embedding 将一个视觉问题转化为了一个 seq2seq 问题。 positional encoding（standard learnable 1D position embeddings）：ViT 同样需要加入位置编码，位置编码可以理解为一张表，表一共有 N 行，N 的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列 embedding 的维度相同（768）。注意位置编码的操作是 sum，而不是 concat。加入位置编码信息之后，维度依然是 197x768。 LN/multi-head attention/LN：LN输出维度依然是 197x768。多头自注意力时，先将输入映射到 q，k，v，如果只有一个头，qkv 的维度都是197x768，如果有 12 个头（768/12=64），则 qkv 的维度是 197x64，一共有 12 组 qkv，最后再将 12 组 qkv 的输出拼接起来，输出维度是 197x768，然后在过一层 LN，维度依然是197x768。 MLP：将维度放大再缩小回去，197x768 放大为 197x3072，再缩小变为 197x768。  一个 block 之后维度依然和输入相同，都是 197x768，因此可以堆叠多个 block。最后会将特殊字符 cls 对应的输出 $Z_L^0$ 作为 encoder 的最终输出 ，代表最终的 image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），如下公式(4)，后面接一个MLP进行图片分类： $$ \begin{aligned} \mathbf{z}{0} &amp; =\left[\mathbf{x}{\text {class }} ; \mathbf{x}{p}^{1} \mathbf{E} ; \mathbf{x}{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}{p}^{N} \mathbf{E}\right]&#43;\mathbf{E}{\text {pos }}, &amp; &amp; \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}{\text {pos }} \in \mathbb{R}^{(N&#43;1) \times D} \ \mathbf{z}{\ell}^{\prime} &amp; =\operatorname{MSA}\left(\mathrm{LN}\left(\mathbf{z}{\ell-1}\right)\right)&#43;\mathbf{z}{\ell-1}, &amp; &amp; \ell=1 \ldots L \ \mathbf{z}{\ell} &amp; =\operatorname{MLP}\left(\mathrm{LN}\left(\mathbf{z}{\ell}^{\prime}\right)\right)&#43;\mathbf{z}{\ell}^{\prime}, &amp; &amp; \ell=1 \ldots L \ \mathbf{y} &amp; =\operatorname{LN}\left(\mathbf{z}{L}^{0}\right) &amp; &amp; \end{aligned} $$ 其中输入 image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ ，2D patches $\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$ ， $C$ 是通道数，$P$ 是 patch 大小，一共有 $N$ 个 patches，$N = HW/P^2$ 。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://perhapzz.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "医疗影像网络架构学习",
      "item": "https://perhapzz.github.io/posts/medicalimage/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "医疗影像网络架构学习",
  "name": "医疗影像网络架构学习",
  "description": "ViT Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Paper: arXiv:2010.11929\n 把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。\nViT 结构 按照上面的流程图，一个 ViT block 可以分为以下几个步骤:\n patch embedding：例如输入图片大小为 224x224，将图片分为固定大小的 patch，patch 大小为 16x16，则每张图像会生成 224x224/16x16=196 个 patch，即输入序列长度为 196，每个patch维度 16x16x3=768，线性投射层的维度为 768xN (N=768)，因此输入通过线性投射层之后的维度依然为 196x768，即一共有 196 个 token，每个 token 的维度是 768。这里还需要加上一个特殊字符 cls（类似 BERT），因此最终的维度是 197x768。到目前为止，已经通过 patch embedding 将一个视觉问题转化为了一个 seq2seq 问题。 positional encoding（standard learnable 1D position embeddings）：ViT 同样需要加入位置编码，位置编码可以理解为一张表，表一共有 N 行，N 的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列 embedding 的维度相同（768）。注意位置编码的操作是 sum，而不是 concat。加入位置编码信息之后，维度依然是 197x768。 LN/multi-head attention/LN：LN输出维度依然是 197x768。多头自注意力时，先将输入映射到 q，k，v，如果只有一个头，qkv 的维度都是197x768，如果有 12 个头（768/12=64），则 qkv 的维度是 197x64，一共有 12 组 qkv，最后再将 12 组 qkv 的输出拼接起来，输出维度是 197x768，然后在过一层 LN，维度依然是197x768。 MLP：将维度放大再缩小回去，197x768 放大为 197x3072，再缩小变为 197x768。  一个 block 之后维度依然和输入相同，都是 197x768，因此可以堆叠多个 block。最后会将特殊字符 cls 对应的输出 $Z_L^0$ 作为 encoder 的最终输出 ，代表最终的 image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），如下公式(4)，后面接一个MLP进行图片分类： $$ \\begin{aligned} \\mathbf{z}{0} \u0026amp; =\\left[\\mathbf{x}{\\text {class }} ; \\mathbf{x}{p}^{1} \\mathbf{E} ; \\mathbf{x}{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}{\\text {pos }}, \u0026amp; \u0026amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\ \\mathbf{z}{\\ell}^{\\prime} \u0026amp; =\\operatorname{MSA}\\left(\\mathrm{LN}\\left(\\mathbf{z}{\\ell-1}\\right)\\right)+\\mathbf{z}{\\ell-1}, \u0026amp; \u0026amp; \\ell=1 \\ldots L \\ \\mathbf{z}{\\ell} \u0026amp; =\\operatorname{MLP}\\left(\\mathrm{LN}\\left(\\mathbf{z}{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}{\\ell}^{\\prime}, \u0026amp; \u0026amp; \\ell=1 \\ldots L \\ \\mathbf{y} \u0026amp; =\\operatorname{LN}\\left(\\mathbf{z}{L}^{0}\\right) \u0026amp; \u0026amp; \\end{aligned} $$ 其中输入 image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ ，2D patches $\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$ ， $C$ 是通道数，$P$ 是 patch 大小，一共有 $N$ 个 patches，$N = HW/P^2$ 。",
  "keywords": [
    
  ],
  "articleBody": "ViT Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale Paper: arXiv:2010.11929\n 把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。\nViT 结构 按照上面的流程图，一个 ViT block 可以分为以下几个步骤:\n patch embedding：例如输入图片大小为 224x224，将图片分为固定大小的 patch，patch 大小为 16x16，则每张图像会生成 224x224/16x16=196 个 patch，即输入序列长度为 196，每个patch维度 16x16x3=768，线性投射层的维度为 768xN (N=768)，因此输入通过线性投射层之后的维度依然为 196x768，即一共有 196 个 token，每个 token 的维度是 768。这里还需要加上一个特殊字符 cls（类似 BERT），因此最终的维度是 197x768。到目前为止，已经通过 patch embedding 将一个视觉问题转化为了一个 seq2seq 问题。 positional encoding（standard learnable 1D position embeddings）：ViT 同样需要加入位置编码，位置编码可以理解为一张表，表一共有 N 行，N 的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列 embedding 的维度相同（768）。注意位置编码的操作是 sum，而不是 concat。加入位置编码信息之后，维度依然是 197x768。 LN/multi-head attention/LN：LN输出维度依然是 197x768。多头自注意力时，先将输入映射到 q，k，v，如果只有一个头，qkv 的维度都是197x768，如果有 12 个头（768/12=64），则 qkv 的维度是 197x64，一共有 12 组 qkv，最后再将 12 组 qkv 的输出拼接起来，输出维度是 197x768，然后在过一层 LN，维度依然是197x768。 MLP：将维度放大再缩小回去，197x768 放大为 197x3072，再缩小变为 197x768。  一个 block 之后维度依然和输入相同，都是 197x768，因此可以堆叠多个 block。最后会将特殊字符 cls 对应的输出 $Z_L^0$ 作为 encoder 的最终输出 ，代表最终的 image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），如下公式(4)，后面接一个MLP进行图片分类： $$ \\begin{aligned} \\mathbf{z}{0} \u0026 =\\left[\\mathbf{x}{\\text {class }} ; \\mathbf{x}{p}^{1} \\mathbf{E} ; \\mathbf{x}{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}{\\text {pos }}, \u0026 \u0026 \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\ \\mathbf{z}{\\ell}^{\\prime} \u0026 =\\operatorname{MSA}\\left(\\mathrm{LN}\\left(\\mathbf{z}{\\ell-1}\\right)\\right)+\\mathbf{z}{\\ell-1}, \u0026 \u0026 \\ell=1 \\ldots L \\ \\mathbf{z}{\\ell} \u0026 =\\operatorname{MLP}\\left(\\mathrm{LN}\\left(\\mathbf{z}{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}{\\ell}^{\\prime}, \u0026 \u0026 \\ell=1 \\ldots L \\ \\mathbf{y} \u0026 =\\operatorname{LN}\\left(\\mathbf{z}{L}^{0}\\right) \u0026 \u0026 \\end{aligned} $$ 其中输入 image $\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$ ，2D patches $\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$ ， $C$ 是通道数，$P$ 是 patch 大小，一共有 $N$ 个 patches，$N = HW/P^2$ 。\ntricks 关于CNN+Transformer 既然 CNN 具有归纳偏置的特性，Transformer 又具有很强全局归纳建模能力，使用 CNN+Transformer 的混合模型是不是可以得到更好的效果呢？将 224x224 图片送入 CNN 得到 16x16 的特征图，拉成一个向量，长度为 196，后续操作和 ViT 相同。\n关于输入图片大小 通常在一个很大的数据集上预训练 ViT，然后在下游任务相对小的数据集上微调，已有研究表明在分辨率更高的图片上微调比在在分辨率更低的图片上预训练效果更好。\n当输入图片分辨率发生变化，输入序列的长度也发生变化，虽然ViT可以处理任意长度的序列，但是预训练好的位置编码无法再使用（例如原来是 3x3，一种 9 个 patch，每个 patch 的位置编码都是有明确意义的，如果 patch 数量变多，位置信息就会发生变化），一种做法是使用插值算法，扩大位置编码表。但是如果序列长度变化过大，插值操作会损失模型性能，这是 ViT 在微调时的一种局限性。\n结论 当数据集很小时，CNN 预训练模型表现更好，证明了 CNN 归纳偏置的有效性，但是当数据集足够大时，归纳偏置和 Transformer 比较就失去了优势，甚至没有归纳偏置，直接从数据 learn patterns 会更有效。\nSwin-Transformer Title: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows Paper: arXiv:2103.14030\n 问题  视觉实体变化大，在不同场景下视觉 Transformer 性能未必很好   图像分辨率高，像素点多，Transformer 基于全局自注意力的计算导致计算量较大  解决方案 针对上述两个问题，提出了一种包含滑窗操作，具有层级设计的 Swin Transformer。\n其中滑窗操作包括不重叠的local window，和重叠的cross-window。将注意力计算限制在一个窗口中，一方面能引入CNN卷积操作的局部性，另一方面能节省计算量。\n整体架构 整个模型采取层次化的设计，一共包含 4 个 Stage，每个 Stage 都会缩小输入特征图的分辨率，像 CNN 一样逐层扩大感受野。\n 在输入开始的时候，做了一个Patch Embedding，将图片切成一个个图块，并嵌入到Embedding。 在每个 Stage 里，由Patch Merging和多个Block组成。 其中Patch Merging模块主要在每个 Stage 一开始降低图片分辨率。 而 Block 具体结构如右图所示，主要是LayerNorm，MLP，Window Attention 和 Shifted Window Attention组成 (为了方便讲解，我会省略掉一些参数)  其中有几个地方处理方法与ViT不同：\n ViT 在输入会给 embedding 进行位置编码。而 Swin-T 这里则是作为一个可选项（self.ape），Swin-T 是在计算 Attention 的时候做了一个相对位置编码 ViT 会单独加上一个可学习参数，作为分类的 token。而 Swin-T 则是直接做平均，输出分类，有点类似 CNN 最后的全局平均池化层  组件 Patch Embedding 在输入进 Block 前，我们需要将图片切成一个个 patch，然后嵌入向量。具体做法是对原始图片裁成一个个 patch_size * patch_size的窗口大小，然后进行嵌入。\n这里可以通过二维卷积层，将 stride，kernelsize 设置为 patch_size 大小。设定输出通道来确定嵌入向量的大小。最后将 H,W 维度展开，并移动到第一维度\nPatch Merging 该模块的作用是在每个 Stage 开始前做降采样，用于缩小分辨率，调整通道数进而形成层次化的设计，同时也能节省一定运算量。\n 在CNN中，则是在每个Stage开始前用stride=2的卷积/池化层来降低分辨率。\n 每次降采样是两倍，因此在行方向和列方向上，间隔2选取元素。\n然后拼接在一起作为一整个张量，最后展开。此时通道维度会变成原先的4倍（因为H,W各缩小2倍），此时再通过一个全连接层再调整通道维度为原来的两倍\n示意图：\nWindow Partition/Reverse window partition 函数是用于对张量划分窗口，指定窗口大小。将原本的张量从 N H W C, 划分成 num_windows*B, window_size, window_size, C，其中 num_windows = H*W / (window_size*window_size)，即窗口的个数。而window reverse函数则是对应的逆过程。这两个函数会在后面的 Window Attention 用到。\nWindow Attention 这是这篇文章的关键。传统的 Transformer 都是基于全局来计算注意力的，因此计算复杂度十分高。而 Swin Transformer 则将注意力的计算限制在每个窗口内，进而减少了计算量。\n我们先简单看下公式： $$ Attention(Q,K,V) = Softmax(\\frac{QK^T}{\\sqrt{d}}+B)V $$ 主要区别是在原始计算Attention的公式中的Q,K时加入了相对位置编码。后续实验有证明相对位置编码的加入提升了模型性能。\nShift Window Attention 前面的 Window Attention 是在每个窗口下计算注意力的，为了更好的和其他 Window 进行信息交互，Swin Transformer还引入了 Shifted Window 操作。\n 将训练 Epoch 数从 90 增加到 300 优化器从 SGD 改为 AdamW 更复杂的数据扩充策略，包括 Mixup、CutMix、RandAugment、Random Erasing 等 增加正则策略，例如随机深度[7]，标签平滑[8]，EMA[9]等  ConvNet Title: A ConvNet for the 2020s Paper: arXiv:2201.03545\n 3D UX-Net Title: 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation Paper: arXiv:2209.15076\n MedNeXt Title: MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation Paper: https://arxiv.org/pdf/2303.0997\n 结合 ConvNeXT 的新一代 Transformer 网络架构\n",
  "wordCount" : "442",
  "inLanguage": "en",
  "datePublished": "2023-03-22T21:19:02+08:00",
  "dateModified": "2023-03-22T21:19:02+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://perhapzz.github.io/posts/medicalimage/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "perhapzz",
    "logo": {
      "@type": "ImageObject",
      "url": "https://perhapzz.github.io/favicon/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://perhapzz.github.io" accesskey="h" title="perhapzz (Alt + H)">perhapzz</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://perhapzz.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://perhapzz.github.io/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://perhapzz.github.io">Home</a>&nbsp;»&nbsp;<a href="https://perhapzz.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      医疗影像网络架构学习
    </h1>
    <div class="post-meta"><span title='2023-03-22 21:19:02 +0800 CST'>March 22, 2023</span>

</div>
  </header> 
  <div class="post-content"><h1 id="vit">ViT<a hidden class="anchor" aria-hidden="true" href="#vit">#</a></h1>
<p><strong>Title</strong>: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
<strong>Paper</strong>: <a href="https://arxiv.org/abs/2010.11929">arXiv:2010.11929</a></p>
<hr>
<p>把最重要的说在最前面，ViT原论文中最核心的结论是，当拥有足够多的数据进行预训练的时候，ViT的表现就会超过CNN，突破transformer缺少归纳偏置的限制，可以在下游任务中获得较好的迁移效果。</p>
<h2 id="vit-结构">ViT 结构<a hidden class="anchor" aria-hidden="true" href="#vit-结构">#</a></h2>
<p><img loading="lazy" src="https://raw.githubusercontent.com/perhapzz/ImageBed/main/blog-images/202303271652494.png" alt="image-20230326153708733"  />
</p>
<p>按照上面的流程图，一个 ViT block 可以分为以下几个步骤:</p>
<ol>
<li><strong>patch embedding</strong>：例如输入图片大小为 224x224，将图片分为固定大小的 patch，patch 大小为 16x16，则每张图像会生成 224x224/16x16=196 个 patch，即输入序列长度为 <strong>196</strong>，每个patch维度 16x16x3=<strong>768</strong>，线性投射层的维度为 768xN (N=768)，因此输入通过线性投射层之后的维度依然为 196x768，即一共有 196 个 token，每个 token 的维度是 768。这里还需要加上一个特殊字符 cls（类似 BERT），因此最终的维度是 <strong>197x768</strong>。到目前为止，已经通过 patch embedding 将一个视觉问题转化为了一个 seq2seq 问题。</li>
<li><strong>positional encoding（standard learnable 1D position embeddings）</strong>：ViT 同样需要加入位置编码，位置编码可以理解为一张表，表一共有 N 行，N 的大小和输入序列长度相同，每一行代表一个向量，向量的维度和输入序列 embedding 的维度相同（768）。注意位置编码的操作是 sum，而不是 concat。加入位置编码信息之后，维度依然是 <strong>197x768</strong>。</li>
<li><strong>LN/multi-head attention/LN</strong>：LN输出维度依然是 197x768。多头自注意力时，先将输入映射到 q，k，v，如果只有一个头，qkv 的维度都是197x768，如果有 12 个头（768/12=64），则 qkv 的维度是 197x64，一共有 12 组 qkv，最后再将 12 组 qkv 的输出拼接起来，输出维度是 197x768，然后在过一层 LN，维度依然是<strong>197x768</strong>。</li>
<li><strong>MLP</strong>：将维度放大再缩小回去，197x768 放大为 197x3072，再缩小变为 <strong>197x768</strong>。</li>
</ol>
<p>一个 block 之后维度依然和输入相同，都是 197x768，因此可以堆叠多个 block。最后会将特殊字符 cls 对应的输出  $Z_L^0$ 作为 encoder 的最终输出 ，代表最终的 image presentation（另一种做法是不加cls字符，对所有的tokens的输出做一个平均），如下公式(4)，后面接一个MLP进行图片分类：
$$
\begin{aligned}
\mathbf{z}<em>{0} &amp; =\left[\mathbf{x}</em>{\text {class }} ; \mathbf{x}<em>{p}^{1} \mathbf{E} ; \mathbf{x}</em>{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}<em>{p}^{N} \mathbf{E}\right]+\mathbf{E}</em>{\text {pos }}, &amp; &amp; \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}<em>{\text {pos }} \in \mathbb{R}^{(N+1) \times D} \
\mathbf{z}</em>{\ell}^{\prime} &amp; =\operatorname{MSA}\left(\mathrm{LN}\left(\mathbf{z}<em>{\ell-1}\right)\right)+\mathbf{z}</em>{\ell-1}, &amp; &amp; \ell=1 \ldots L \
\mathbf{z}<em>{\ell} &amp; =\operatorname{MLP}\left(\mathrm{LN}\left(\mathbf{z}</em>{\ell}^{\prime}\right)\right)+\mathbf{z}<em>{\ell}^{\prime}, &amp; &amp; \ell=1 \ldots L \
\mathbf{y} &amp; =\operatorname{LN}\left(\mathbf{z}</em>{L}^{0}\right) &amp; &amp;
\end{aligned}
$$
其中输入 image $\mathbf{x} \in \mathbb{R}^{H \times W \times C}$ ，2D patches $\mathbf{x}_{p} \in \mathbb{R}^{N \times\left(P^{2} \cdot C\right)}$ ， $C$  是通道数，$P$ 是 patch 大小，一共有 $N$ 个 patches，$N = HW/P^2$ 。</p>
<h2 id="tricks">tricks<a hidden class="anchor" aria-hidden="true" href="#tricks">#</a></h2>
<h3 id="关于cnntransformer">关于CNN+Transformer<a hidden class="anchor" aria-hidden="true" href="#关于cnntransformer">#</a></h3>
<p>既然 CNN 具有归纳偏置的特性，Transformer 又具有很强全局归纳建模能力，使用 CNN+Transformer 的混合模型是不是可以得到更好的效果呢？将 224x224 图片送入 CNN 得到 16x16 的特征图，拉成一个向量，长度为 196，后续操作和 ViT 相同。</p>
<h3 id="关于输入图片大小">关于输入图片大小<a hidden class="anchor" aria-hidden="true" href="#关于输入图片大小">#</a></h3>
<p>通常在一个很大的数据集上预训练 ViT，然后在下游任务相对小的数据集上微调，已有研究表明在分辨率更高的图片上微调比在在分辨率更低的图片上预训练效果更好。</p>
<p>当输入图片分辨率发生变化，输入序列的长度也发生变化，虽然ViT可以处理任意长度的序列，但是预训练好的位置编码无法再使用（例如原来是 3x3，一种 9 个 patch，每个 patch 的位置编码都是有明确意义的，如果 patch 数量变多，位置信息就会发生变化），一种做法是使用插值算法，扩大位置编码表。但是如果序列长度变化过大，插值操作会损失模型性能，这是 ViT 在微调时的一种局限性。</p>
<h2 id="结论">结论<a hidden class="anchor" aria-hidden="true" href="#结论">#</a></h2>
<p>当数据集很小时，CNN 预训练模型表现更好，证明了 CNN 归纳偏置的有效性，但是当数据集足够大时，归纳偏置和 Transformer 比较就失去了优势，甚至没有归纳偏置，直接从数据 learn patterns 会更有效。</p>
<h1 id="swin-transformer">Swin-Transformer<a hidden class="anchor" aria-hidden="true" href="#swin-transformer">#</a></h1>
<p><strong>Title</strong>: Swin Transformer: Hierarchical Vision Transformer using Shifted Windows
<strong>Paper</strong>: <a href="https://arxiv.org/abs/2103.14030">arXiv:2103.14030</a></p>
<hr>
<h2 id="问题">问题<a hidden class="anchor" aria-hidden="true" href="#问题">#</a></h2>
<ul>
<li>视觉实体变化大，在不同场景下视觉 Transformer 性能未必很好</li>
</ul>
<ul>
<li>图像分辨率高，像素点多，Transformer 基于全局自注意力的计算导致计算量较大</li>
</ul>
<h2 id="解决方案">解决方案<a hidden class="anchor" aria-hidden="true" href="#解决方案">#</a></h2>
<p>针对上述两个问题，提出了一种<strong>包含滑窗操作，具有层级设计</strong>的 Swin Transformer。</p>
<p>其中滑窗操作包括<strong>不重叠的local window，和重叠的cross-window</strong>。将注意力计算限制在一个窗口中，<strong>一方面能引入CNN卷积操作的局部性，另一方面能节省计算量</strong>。</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/perhapzz/ImageBed/main/blog-images/202303222302607.png" alt="image-20230322230244259"  />
</p>
<h2 id="整体架构">整体架构<a hidden class="anchor" aria-hidden="true" href="#整体架构">#</a></h2>
<p><img loading="lazy" src="https://raw.githubusercontent.com/perhapzz/ImageBed/main/blog-images/202303222253262.png" alt="image-20230322225358206"  />
</p>
<p>整个模型采取层次化的设计，一共包含 4 个 Stage，每个 Stage 都会缩小输入特征图的分辨率，像 CNN 一样逐层扩大感受野。</p>
<ul>
<li>在输入开始的时候，做了一个<code>Patch Embedding</code>，将图片切成一个个图块，并嵌入到<code>Embedding</code>。</li>
<li>在每个 Stage 里，由<code>Patch Merging</code>和多个Block组成。</li>
<li>其中<code>Patch Merging</code>模块主要在每个 Stage 一开始降低图片分辨率。</li>
<li>而 Block 具体结构如右图所示，主要是<code>LayerNorm</code>，<code>MLP</code>，<code>Window Attention</code> 和 <code>Shifted Window Attention</code>组成 (为了方便讲解，我会省略掉一些参数)</li>
</ul>
<p>其中有几个地方处理方法与ViT不同：</p>
<ul>
<li>ViT 在输入会给 embedding 进行位置编码。而 Swin-T 这里则是作为一个<strong>可选项</strong>（<code>self.ape</code>），Swin-T 是在计算 Attention 的时候做了一个<code>相对位置编码</code></li>
<li>ViT 会单独加上一个可学习参数，作为分类的 token。而 Swin-T 则是<strong>直接做平均</strong>，输出分类，有点类似 CNN 最后的全局平均池化层</li>
</ul>
<h2 id="组件">组件<a hidden class="anchor" aria-hidden="true" href="#组件">#</a></h2>
<h3 id="patch-embedding">Patch Embedding<a hidden class="anchor" aria-hidden="true" href="#patch-embedding">#</a></h3>
<p>在输入进 Block 前，我们需要将图片切成一个个 patch，然后嵌入向量。具体做法是对原始图片裁成一个个 <code>patch_size * patch_size</code>的窗口大小，然后进行嵌入。</p>
<p>这里可以通过二维卷积层，<strong>将 stride，kernelsize 设置为 patch_size 大小</strong>。设定输出通道来确定嵌入向量的大小。最后将 H,W 维度展开，并移动到第一维度</p>
<h3 id="patch-merging">Patch Merging<a hidden class="anchor" aria-hidden="true" href="#patch-merging">#</a></h3>
<p>该模块的作用是在每个 Stage 开始前做降采样，用于缩小分辨率，调整通道数进而形成层次化的设计，同时也能节省一定运算量。</p>
<blockquote>
<p>在CNN中，则是在每个Stage开始前用<code>stride=2</code>的卷积/池化层来降低分辨率。</p>
</blockquote>
<p>每次降采样是两倍，因此<strong>在行方向和列方向上，间隔2选取元素</strong>。</p>
<p>然后拼接在一起作为一整个张量，最后展开。<strong>此时通道维度会变成原先的4倍</strong>（因为H,W各缩小2倍），此时再通过一个<strong>全连接层再调整通道维度为原来的两倍</strong></p>
<p><strong>示意图：</strong></p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/perhapzz/ImageBed/main/blog-images/202303222334250.png" alt="image-20230322233419549"  />
</p>
<h3 id="window-partitionreverse">Window Partition/Reverse<a hidden class="anchor" aria-hidden="true" href="#window-partitionreverse">#</a></h3>
<p><code>window partition</code> 函数是用于对张量划分窗口，指定窗口大小。将原本的张量从 <code>N H W C</code>, 划分成 <code>num_windows*B, window_size, window_size, C</code>，其中 <strong>num_windows = H*W / (window_size*window_size)</strong>，即窗口的个数。而<code>window reverse</code>函数则是对应的逆过程。这两个函数会在后面的 <code>Window Attention</code> 用到。</p>
<h3 id="window-attention">Window Attention<a hidden class="anchor" aria-hidden="true" href="#window-attention">#</a></h3>
<p>这是这篇文章的关键。传统的 Transformer 都是<strong>基于全局来计算注意力的</strong>，因此计算复杂度十分高。而 Swin Transformer 则将<strong>注意力的计算限制在每个窗口内</strong>，进而减少了计算量。</p>
<p>我们先简单看下公式：
$$
Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d}}+B)V
$$
主要区别是在原始计算Attention的公式中的Q,K时<strong>加入了相对位置编码</strong>。后续实验有证明相对位置编码的加入提升了模型性能。</p>
<h3 id="shift-window-attention">Shift Window Attention<a hidden class="anchor" aria-hidden="true" href="#shift-window-attention">#</a></h3>
<p>前面的 Window Attention 是在每个窗口下计算注意力的，为了更好的和其他 Window 进行信息交互，Swin Transformer还引入了 Shifted Window 操作。</p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/perhapzz/ImageBed/main/blog-images/202303242221414.png" alt="image-20230324222059637"  />
</p>
<ol>
<li>将训练 Epoch 数从 90 增加到 300</li>
<li>优化器从 SGD 改为 AdamW</li>
<li>更复杂的数据扩充策略，包括 Mixup、CutMix、RandAugment、Random Erasing 等</li>
<li>增加正则策略，例如随机深度[7]，标签平滑[8]，EMA[9]等</li>
</ol>
<h1 id="convnet">ConvNet<a hidden class="anchor" aria-hidden="true" href="#convnet">#</a></h1>
<p><strong>Title</strong>: A ConvNet for the 2020s
<strong>Paper</strong>: <a href="https://arxiv.org/abs/2201.03545">arXiv:2201.03545</a></p>
<hr>
<h1 id="3d-ux-net">3D UX-Net<a hidden class="anchor" aria-hidden="true" href="#3d-ux-net">#</a></h1>
<p><strong>Title</strong>: 3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation
<strong>Paper</strong>: <a href="https://arxiv.org/abs/2209.15076">arXiv:2209.15076</a></p>
<hr>
<h1 id="mednext">MedNeXt<a hidden class="anchor" aria-hidden="true" href="#mednext">#</a></h1>
<p><strong>Title</strong>: MedNeXt: Transformer-driven Scaling of ConvNets for Medical Image Segmentation
<strong>Paper</strong>: <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2303.09975.pdf">https://arxiv.org/pdf/2303.0997</a></p>
<hr>
<p><strong>结合 ConvNeXT 的新一代 Transformer 网络架构</strong></p>
<p><img loading="lazy" src="https://raw.githubusercontent.com/perhapzz/ImageBed/main/blog-images/202303222143371.png" alt="image-20230322214259772"  />
</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://perhapzz.github.io">perhapzz</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
